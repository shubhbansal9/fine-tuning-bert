{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde438e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shubh\\anaconda3\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\shubh\\anaconda3\\lib\\site-packages (2.13.2)\n",
      "Requirement already satisfied: torch in c:\\users\\shubh\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shubh\\anaconda3\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shubh\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: peft in c:\\users\\shubh\\anaconda3\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from peft) (5.8.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from peft) (0.31.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from torch) (2.5)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from networkx->torch) (5.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shubh\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch scikit-learn matplotlib peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e99171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\lib\\site-packages\\transformers\\adapters\\__init__.py:27: FutureWarning: The `adapter-transformers` package is deprecated and replaced by the `adapters` package. See https://docs.adapterhub.ml/transitioning.html.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    " import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12528954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 4457\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 1115\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load spam dataset\n",
    "spam_df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "spam_df = spam_df.rename(columns={'v1': 'label', 'v2': 'text'})\n",
    "\n",
    "# Map labels to integers\n",
    "spam_df['label'] = spam_df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Only keep necessary columns\n",
    "spam_df = spam_df[['text', 'label']].dropna()\n",
    "\n",
    "# Check the size of the dataset\n",
    "print(f\"Dataset size: {len(spam_df)}\")\n",
    "\n",
    "# Reduce dataset size for quicker training if the dataset is large\n",
    "if len(spam_df) > 10000:\n",
    "    spam_df = spam_df.sample(n=10000, random_state=42)\n",
    "\n",
    "dataset = Dataset.from_pandas(spam_df)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd388fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define tokenizer\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Function to tokenize data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b1acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)  # Convert logits to tensor\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8358d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Change from eval_strategy to evaluation_strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b50a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(model, training_args, tokenized_datasets):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    eval_results['train_time'] = train_time\n",
    "    eval_results['eval_time'] = eval_time\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd6332cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\shubh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4457\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 279\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 50:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.039201</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.965753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1115\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1115\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 04:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Results: {'eval_loss': 0.039200883358716965, 'eval_accuracy': 0.9919282511210762, 'eval_f1': 0.9690721649484535, 'eval_precision': 0.9724137931034482, 'eval_recall': 0.9657534246575342, 'eval_runtime': 250.0497, 'eval_samples_per_second': 4.459, 'eval_steps_per_second': 0.28, 'epoch': 1.0, 'train_time': 3047.671599626541, 'eval_time': 250.059711933136}\n"
     ]
    }
   ],
   "source": [
    "# Base Model without Soft Prompts or LoRA\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "base_results = train_and_evaluate(base_model, training_args, tokenized_datasets)\n",
    "print(\"Base Model Results:\", base_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e2165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SoftPromptModel(AutoModelForSequenceClassification):\n",
    "    def __init__(self, config, num_labels, soft_prompt_length=20):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.soft_prompt_length = soft_prompt_length\n",
    "        self.soft_prompt_embed = nn.Embedding(self.soft_prompt_length, config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        soft_prompt=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82314a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\shubh/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\shubh/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\shubh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4457\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 279\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 48:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.034612</td>\n",
       "      <td>0.990135</td>\n",
       "      <td>0.962457</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.965753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1115\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1115\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 03:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Prompt Results: {'eval_loss': 0.03461229056119919, 'eval_accuracy': 0.9901345291479821, 'eval_f1': 0.962457337883959, 'eval_precision': 0.9591836734693877, 'eval_recall': 0.9657534246575342, 'eval_runtime': 227.9528, 'eval_samples_per_second': 4.891, 'eval_steps_per_second': 0.307, 'epoch': 1.0, 'train_time': 2910.8580236434937, 'eval_time': 227.95892429351807}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "soft_prompt_model = SoftPromptModel.from_pretrained(model_checkpoint, num_labels=2)\n",
    "soft_prompt_results = train_and_evaluate(soft_prompt_model, training_args, tokenized_datasets)\n",
    "print(\"Soft Prompt Results:\", soft_prompt_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "659f2c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\shubh/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\shubh/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\shubh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4457\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 279\n",
      "  Number of trainable parameters = 739586\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 30:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>0.871749</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1115\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1115\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 02:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Model Results: {'eval_loss': 0.2000962644815445, 'eval_accuracy': 0.8717488789237668, 'eval_f1': 0.040268456375838924, 'eval_precision': 1.0, 'eval_recall': 0.02054794520547945, 'eval_runtime': 147.3047, 'eval_samples_per_second': 7.569, 'eval_steps_per_second': 0.475, 'epoch': 1.0, 'train_time': 1823.58278465271, 'eval_time': 147.31470489501953}\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    r=8,                        # Low rank\n",
    "    lora_alpha=32,              # Alpha parameter for LoRA\n",
    "    lora_dropout=0.1,           # Dropout for LoRA\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"v_lin\"] \n",
    ")\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "lora_model = get_peft_model(AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2), lora_config)\n",
    "\n",
    "# Train and evaluate the LoRA model\n",
    "lora_results = train_and_evaluate(lora_model, training_args, tokenized_datasets)\n",
    "print(\"LoRA Model Results:\", lora_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6857ac4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "labels = [\"Base Model\", \"Soft Prompts\", \"LoRA\"]\n",
    "accuracy = [base_results.get(\"eval_accuracy\", 0), soft_prompt_results.get(\"eval_accuracy\", 0), lora_results.get(\"eval_accuracy\", 0)]\n",
    "f1 = [base_results.get(\"eval_f1\", 0), soft_prompt_results.get(\"eval_f1\", 0), lora_results.get(\"eval_f1\", 0)]\n",
    "precision = [base_results.get(\"eval_precision\", 0), soft_prompt_results.get(\"eval_precision\", 0), lora_results.get(\"eval_precision\", 0)]\n",
    "recall = [base_results.get(\"eval_recall\", 0), soft_prompt_results.get(\"eval_recall\", 0), lora_results.get(\"eval_recall\", 0)]\n",
    "train_time = [base_results.get(\"train_time\", 0), soft_prompt_results.get(\"train_time\", 0), lora_results.get(\"train_time\", 0)]\n",
    "eval_time = [base_results.get(\"eval_time\", 0), soft_prompt_results.get(\"eval_time\", 0), lora_results.get(\"eval_time\", 0)]\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axs[0, 0].bar(labels, accuracy, color='b')\n",
    "axs[0, 0].set_title('Accuracy')\n",
    "axs[0, 1].bar(labels, f1, color='g')\n",
    "axs[0, 1].set_title('F1 Score')\n",
    "axs[1, 0].bar(labels, precision, color='r')\n",
    "axs[1, 0].set_title('Precision')\n",
    "axs[1, 1].bar(labels, recall, color='c')\n",
    "axs[1, 1].set_title('Recall')\n",
    "axs[2, 0].bar(labels, train_time, color='m')\n",
    "axs[2, 0].set_title('Training Time')\n",
    "axs[2, 1].bar(labels, eval_time, color='y')\n",
    "axs[2, 1].set_title('Evaluation Time')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('Model Type')\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a24bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
